{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second stage model training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities of different outcomes output from the first model might be hierarchically related (due to nonindependence from the species tree).  \n",
    "This second model trains on the predicted probabilities from the first model to improve inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import simcat\n",
    "import toytree\n",
    "import toyplot\n",
    "import toyplot.svg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import ipcoal\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up our trained model from the first stage of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = load_model(\"../models/bal_10tip_2mil/firststage_mod.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up the original simulated training data again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] cleaned\n",
      "[load] (63740, 210, 16, 16)\n",
      "[filter] (63740, 210, 16, 16)\n",
      "[vectorize] (63740, 53760)\n",
      "[train/test] (42705, 53760)/(21035, 53760)\n"
     ]
    }
   ],
   "source": [
    "mod = simcat.Analysis(\n",
    "    name=\"cleaned\",\n",
    "    workdir=\"../merged/\",\n",
    "    mask_admixture_min=0.04,\n",
    "    mask_sisters=True,\n",
    "    scale=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train/test] (57366, 53760)/(6374, 53760)\n"
     ]
    }
   ],
   "source": [
    "mod.train_test_split(prop=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-encode the labels for use by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels as ints:\n",
    "unique_labs = np.unique(mod.y)\n",
    "onehot_dict = dict(zip(range(len(unique_labs)),unique_labs))\n",
    "inv_onehot_dict = dict(zip(unique_labs,range(len(unique_labs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of non-sister admixture scenarios in our training data\n",
    "len(onehot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode training labels\n",
    "y_idxs = [inv_onehot_dict[i] for i in np.array(mod.y_train)]\n",
    "y = np.zeros((len(y_idxs),len(onehot_dict)))\n",
    "for rowidx in range(y.shape[0]):\n",
    "    y[rowidx,y_idxs[rowidx]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode test labels\n",
    "y_test_idxs = [inv_onehot_dict[i] for i in np.array(mod.y_test)]\n",
    "y_test = np.zeros((len(y_test_idxs),len(onehot_dict)))\n",
    "for rowidx in range(y_test.shape[0]):\n",
    "    y_test[rowidx,y_test_idxs[rowidx]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, we want to exclude introgression between sister edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for excluding NaN from the analysis -- which integer value is NaN?\n",
    "nanval = {onehot_dict[i]:i for i in onehot_dict.keys()}[\"NaN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's make predictions from the simulated training data with the first model stage.  \n",
    "### These predictions are the training data for our second stage model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preds = nn_model.predict(mod.X_train[~(np.argmax(y,1)==nanval)])\n",
    "training_y = y[~(np.argmax(y,1)==nanval)]\n",
    "testing_preds = nn_model.predict(mod.X_test[~(np.argmax(y_test,1)==nanval)])\n",
    "testing_y = y_test[~(np.argmax(y_test,1)==nanval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5649, 177)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5649, 177)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50808, 177)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50808, 177)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the second stage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=training_preds.shape[1], activation='relu'))\n",
    "#model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(training_y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model, reporting results on a separate test dataset along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~ Training epoch 0: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.67\n",
      "test accuracy: 0.73\n",
      "~~~~~~~~~~~~~~ Training epoch 1: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.77\n",
      "test accuracy: 0.8\n",
      "~~~~~~~~~~~~~~ Training epoch 2: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.8\n",
      "test accuracy: 0.81\n",
      "~~~~~~~~~~~~~~ Training epoch 3: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.81\n",
      "test accuracy: 0.81\n",
      "~~~~~~~~~~~~~~ Training epoch 4: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.81\n",
      "test accuracy: 0.81\n",
      "~~~~~~~~~~~~~~ Training epoch 5: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.81\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 6: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.81\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 7: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.81\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 8: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.81\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 9: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 10: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 11: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 12: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 13: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 14: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 15: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 16: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 17: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 18: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 19: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 20: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 21: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 22: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 23: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 24: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 25: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 26: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 27: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 28: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 29: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 30: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 31: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 32: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.82\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 33: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 34: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 35: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 36: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 37: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 38: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 39: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 40: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 41: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 42: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 43: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 44: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 45: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 46: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 47: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 48: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 49: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 50: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 51: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 52: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 53: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 54: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 55: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 56: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 57: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 58: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 59: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 60: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 61: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 62: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 63: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 64: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 65: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 66: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 67: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 68: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 69: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 70: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 71: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 72: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 73: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 74: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.84\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 75: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 76: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.83\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 77: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.84\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 78: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.84\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 79: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.84\n",
      "test accuracy: 0.82\n",
      "~~~~~~~~~~~~~~ Training epoch 80: ~~~~~~~~~~~~~~\n",
      "training accuracy: 0.84\n",
      "test accuracy: 0.83\n",
      "~~~~~~~~~~~~~~ Training epoch 81: ~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-69a5fa8c885f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                         verbose=False)\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "epoch_accuracies = []\n",
    "test_accuracies = []\n",
    "for i in range(num_epochs):\n",
    "    print(\"~~~~~~~~~~~~~~ Training epoch \"+ str(i) + \": ~~~~~~~~~~~~~~\")\n",
    "    history = model.fit(training_preds, \n",
    "                        training_y, \n",
    "                        epochs=1, \n",
    "                        batch_size=512,\n",
    "                        verbose=False)\n",
    "    acc = history.history['accuracy']\n",
    "    print(\"training accuracy: \" + str(round(acc[0],2)))\n",
    "    epoch_accuracies.append(acc[0])\n",
    "    \n",
    "    # now make predictions on the test data\n",
    "    y_pred = model.predict(testing_preds)\n",
    "    #Convert predictions to label\n",
    "    pred = list()\n",
    "    for i in range(len(y_pred)):\n",
    "        pred.append(np.argmax(y_pred[i]))\n",
    "        \n",
    "    #Converting one hot encoded test label to label\n",
    "    test = list()\n",
    "    for i in range(len(testing_y)):\n",
    "        test.append(np.argmax(testing_y[i]))\n",
    "        \n",
    "    a = accuracy_score(pred,test)\n",
    "    print(\"test accuracy: \"+str(round(a,2)))\n",
    "    test_accuracies.append(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can see that the model has about 83% success on the test data by the end of model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"secondstage_mod.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
